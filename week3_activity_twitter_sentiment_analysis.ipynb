{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ktanguy/BUMI/blob/main/week3_activity_twitter_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweet-preprocessor"
      ],
      "metadata": {
        "id": "UiLTfPU6NrlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m16i0ikmnTZc"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsWbVIvPnTZx"
      },
      "source": [
        "\"\"\" Importing all libraries \"\"\"\n",
        "\n",
        "import re\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import preprocessor as p\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_th0Dl6enTZ0"
      },
      "source": [
        "## Import Datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Specify the paths to the dataset \"\"\"\n",
        "train_path = None\n",
        "test_path = None"
      ],
      "metadata": {
        "id": "9kPhWIbBM54t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzAEa1_YnTZ1"
      },
      "source": [
        "# training data\n",
        "train = pd.read_csv(train_path)\n",
        "\n",
        "# testing data\n",
        "test = pd.read_csv(test_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpQAH67JnTZ2"
      },
      "source": [
        "## Understanding the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu07b2yTnTZ3"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_oYxgYPnTZ7"
      },
      "source": [
        "test.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a-1Yo5hnTZ8"
      },
      "source": [
        "# good sentiment related tweets\n",
        "sum(train[\"label\"] == 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xfj9rP4vnTZ9"
      },
      "source": [
        "# bad sentiment related tweets\n",
        "sum(train[\"label\"] == 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sFBPsL1nTZ-"
      },
      "source": [
        "# check if there are any missing values\n",
        "train.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G33N10I2nTZ_"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1opNpuS7nTaB"
      },
      "source": [
        "# Set up punctuations we want to be replaced\n",
        "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\|)|(\\()|(\\))|(\\[)|(\\])|(\\%)|(\\$)|(\\>)|(\\<)|(\\{)|(\\})\")\n",
        "REPLACE_WITH_SPACE = re.compile(\"(<br\\s/><br\\s/?)|(-)|(/)|(:).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcWQzz5jnTaC"
      },
      "source": [
        "def clean_tweets(df):\n",
        "    tempArr = []\n",
        "    for line in df:\n",
        "        # clean using tweet_preprocessor\n",
        "        tmpL = p.clean(line)\n",
        "        # remove all punctuation\n",
        "        tmpL = REPLACE_NO_SPACE.sub(\"\", tmpL.lower())\n",
        "        tmpL = REPLACE_WITH_SPACE.sub(\" \", tmpL)\n",
        "        tempArr.append(tmpL)\n",
        "    return tempArr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3d2mWX-nTaD"
      },
      "source": [
        "# clean training data\n",
        "train_tweet = clean_tweets(train[\"tweet\"])\n",
        "train_tweet = pd.DataFrame(train_tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6lFEzKrnTaJ"
      },
      "source": [
        "# append cleaned tweets to the training dataset\n",
        "train[\"clean_tweet\"] = train_tweet\n",
        "\n",
        "# display the new dataset\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgHJd-aYnTaK"
      },
      "source": [
        "# clean training data\n",
        "test_tweet = clean_tweets(test[\"tweet\"])\n",
        "test_tweet = pd.DataFrame(test_tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EoQEP_jnTaM"
      },
      "source": [
        "# append cleaned tweets to the test dataset\n",
        "test[\"clean_tweet\"] = test_tweet\n",
        "\n",
        "# display the new dataset\n",
        "test.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyZWno5dnTaO"
      },
      "source": [
        "## Train and Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZBMKfvCnTaP"
      },
      "source": [
        "# extract the labels from the train data\n",
        "y = train.label.values\n",
        "\n",
        "# use 70% for the training and 30% for the test\n",
        "x_train, x_test, y_train, y_test = train_test_split(train.clean_tweet.values, y,\n",
        "                                                   stratify = y,\n",
        "                                                   random_state = 1,\n",
        "                                                   test_size = 0.3,\n",
        "                                                   shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr_P5TONnTaQ"
      },
      "source": [
        "## Vectorize tweets using CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThHf-D3rnTah"
      },
      "source": [
        "# Implement the Bag of Words vectorization using CountVectorizer()\n",
        "vectorizer = None\n",
        "\n",
        "# learn a vocabulary dictionary of all tokens in the raw documents\n",
        "vectorizer.fit(list(x_train) + list(x_test))\n",
        "\n",
        "# transfrom documents to document-term matrix\n",
        "x_train_vec = vectorizer.transform(x_train)\n",
        "x_test_vec = vectorizer.transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwXyrQmznTan"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG7WFtqunTat"
      },
      "source": [
        "\"\"\" Train a classical machine learning model (like SVC, etc) \"\"\"\n",
        "\n",
        "# Define your model\n",
        "model = None\n",
        "\n",
        "# fit the SVC model based on the given training data\n",
        "model.fit(x_train_vec, y_train).predict_proba(x_test_vec)\n",
        "\n",
        "# perform classification and prediction on samples in x_test\n",
        "y_pred = model.predict(x_test_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkUhYVVCnTay"
      },
      "source": [
        "## Accuracy score for SVC with CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Km69xs1fnTaz"
      },
      "source": [
        "print(\"Accuracy score for SVC is:\", accuracy_score(y_test, y_pred_svm) * 100, \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorize tweets using Tf-IDF"
      ],
      "metadata": {
        "id": "uGTuUoHFPhky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initilizing the TFID Vectorizer()\n",
        "tf_idf_vectorizer = None\n",
        "\n",
        "# learn a vocabulary dictionary of all tokens in the raw documents\n",
        "tf_idf_vectorizer.fit(list(x_train) + list(x_test))\n",
        "\n",
        "# transfrom documents to document-term matrix\n",
        "x_train_vec_tf = tf_idf_vectorizer.transform(x_train)\n",
        "x_test_vec_tf = tf_idf_vectorizer.transform(x_test)"
      ],
      "metadata": {
        "id": "qALYBH-nPkLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your model\n",
        "model = None\n",
        "\n",
        "# fit the SVC model based on the given training data\n",
        "model.fit(x_train_vec_tf, y_train).predict_proba(x_test_vec_tf)\n",
        "\n",
        "# perform classification and prediction on samples in x_test\n",
        "y_pred_svm = model.predict(x_test_vec_tf)"
      ],
      "metadata": {
        "id": "h5_z-jNkPwt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy score for SVC is:\", accuracy_score(y_test, y_pred_svm) * 100, \"%\")"
      ],
      "metadata": {
        "id": "uGWwyzRUP95G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}